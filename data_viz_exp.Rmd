---
title: "Data Visualization Experiments"
author: "Muxuan Lyu & Shuai Shao"
date: "May 4, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# Load packages
library(tidyverse)
library(ggplot2)
library(rvest)
library(dplyr)
library(rgdal)
library(rgeos)
library(maptools)
library(ggthemes)
library(car) 
library(ggpubr)
library(gmodels)
library(knitr)
```

#Introduction

Vision is the most important sense for human beings. Above **70%** of environmental information are accepted by our eyes (Dale Purves, 2008). Data visualization is a complicated process that involves multiple elements, among which the color plays an important role. Color conveys a lot of meaningful information unconsciously. Our brains are always trying to establish the relationship between the color and specific meaning. According to [Lin et al. (2013)]( http://eds.b.ebscohost.com.proxy.uchicago.edu/eds/pdfviewer/pdfviewer?vid=2&sid=6f030dac-b3d6-4b66-b6a2-2aba19836a59%40sessionmgr101), the “associations of colors and meanings” may be grounded in the physical appearance of objects, common metaphors, or other linguistic or cultural conventions. For instance, when visualizing the world map, we always use blue to represent oceans and use green to represent lands. Since it is obvious that human beings have the color bias that relates specific colors and specific meanings, computer programmer set a series of "default value" for data visualization. 

In this project, we plan to conduct an in-depth study to understand whether the color bias exists and how it affects our graphic perception. We will test four specific aspect that prevails in our daily life, the gender-color bias, the emotion-color bias, the political color bias, and the gradient bias. This study will deepen our understanding of color selection, perception, and interpretation in data visualization. 

#Method

##Participants
Participants are recruited on [Amazon Mechanical Turk (MTurk)](https://www.mturk.com/) and they are invited to complete a questionnaire on Qualtrics. As described on the website, "the MTurk web service enables companies to programmatically access this marketplace and a diverse, on-demand workforce and developers can leverage this service to build human intelligence directly into their applications." Participants are required to agree on the consent form at the beginning, and those who complete the survey will receive $0.05 as rewards.

##Procedure 

The order of four studies is randomized among each participant to avoid the order effect. The participants are randomly assigned to one of the congruent condition, incongruent condition, and the control condition within the study. In each study, participants are required to complete two multiple choices according to the graphic. Totally, they have to finish eight questions. Each question lasts five seconds and between two questions there is a two-second break.

##Materials

The questionnaire consists of four studies. 

###Study 1
In Study 1, we aim to test the gender bias. Specifically, we aim to test whether information-receivers have the bias that pink stands for females while blue stands for males. 

In the congruent condition, the graphic displayed is congruent with our hypothetical bias that blue represents males and pink represents females. In the incongruent condition, the graphic displayed is incongruent with our hypothetical bias, and therefore in this condition, blue represents females and pink represents males. In the control condition, we use two colors unrelated to our hypothetical bias to represent the males and females. 

###Study 2
It is widely acknowledged that color is a strong indicator of the emotion [(Kaya & Epps, 200)]( http://eds.b.ebscohost.com.proxy.uchicago.edu/eds/pdfviewer/pdfviewer?vid=2&sid=6f030dac-b3d6-4b66-b6a2-2aba19836a59%40sessionmgr101). Each color may have several associations with different emotions, but we tend to prefer a specific color in the specific mood. In this study, we will test the color-emotion associations. The hypothetical associations are red-angry, dark green-disgust, dark grey-fear, orange-joy, blue-sadness, and purple-surprise [(Manav, 2007)]( https://doi.org/10.1002/col.20294).     

###Study 3
In Study 3, we aim to test the political color bias. In the United States, 
According to [Smithonia Magazine](https://www.smithsonianmag.com/history/when-republicans-were-blue-and-democrats-were-red-104176297/#ixzz2BSTncTH4) and [the Verge](https://www.theverge.com/2012/11/6/3609534/republicans-red-democrats-blue-why-election), it was not until 2000 that red denote the Republican and blue denote the Democrats. It is especially interesting since unlike the emotion color bias which might have some evolutionary meaning, the political color bias is a theoretical "novel bias" spread by social media. This study will tell us whether the political color has been a common bias in human cognition (especially for Americans) and how it affects data visualization. 
In the congruent condition, the graphic displayed is congruent with our hypothetical bias that blue represents the Democrat party and red represents the Republican party. In the incongruent condition, the matching is opposite, in which blue represents Democrats and pink represents Republicans. In the control condition, we use two colors unrelated to our hypothetical bias. 

###Study 4
Gradient graphs are commonly used in geography and [astronomy](https://scitechdaily.com/new-video-maps-the-motions-of-structures-of-the-nearby-universe/). The gradient is typically applied to continuous variables and it denotes the degree or level. However, it remains unclear whether the direction of the color gradient could potentially affect our graph reading. In this study, we aim to test whether the color gradient bias exists.
We use a stimulated density map to test the participants. We hypothesize that human-beings have the bias that the light brightness denotes "lower", "fewer", or "thinner", and correspondingly, dark brightness denotes "higher", "more", or "thicker".

##Analysis

In each study, first, we conduct a Chi-Square test to explore whether the accuracy is significantly different across three conditions. Second, we conduct an ANOVA to test the difference in reacting time of accurate responses. 

#Results
```{r input data}
#get data

all1 <- read.csv("data viz results_rearranged_q1.csv")
all2 <- read.csv("data viz results_rearranged_q2.csv")

gender1 <- all1 %>% select(1:9)
politics1 <- all1 %>% select(10:18)
degree1 <- all1 %>% select(19:27)
emotion1 <- all1 %>% select(28:36)

gender2 <- all2 %>% select(1:9)
politics2 <- all2 %>% select(10:18)
degree2 <- all2 %>% select(19:27)
emotion2 <- all2 %>% select(28:36)
```

```{r reusable "tidy" function}
#reusable tidy data function
tidy <- function(x) {
    #rename levels
    names(x)[c(2,5,8)] = c("con", "incon", "n")
    
    set1 <- x %>% 
      select(2,5,8) %>%
      gather("condition", "value")
    
    set2 <- x %>% 
      select(3,6,9) %>%
      gather("condition", "value")
    
    new <- cbind(set1, set2[,2])
    
    #exclude na values
    new <- new %>% filter(!is.na(value))
    
    new$condition <- as.factor(new$condition)
    names(new)[c(3)] = c("time")
    
    return(new)
}
```

```{r study 1}
#tidy gender data and select the correct answers
gender1_time <- tidy(gender1) %>% filter(value ==2)
gender2_time <- tidy(gender2) %>% filter(value ==2)
gender_time <- rbind(gender1_time, gender2_time)
```


```{r study 1 ANOVA, include=FALSE}
gender_aov = lm(time ~ condition, data = gender_time)
fit_gender = Anova(gender_aov, type = 3)
fit_gender
```


```{r Study 1 Chi-Square}
#chisq
#rerun function without value selection
g1 <- tidy(gender1)
g2 <- tidy(gender2)
gender_new <- rbind(g1, g2)

table_gender = xtabs(~ condition + value, data = gender_new)
kable(table_gender)

chisq.test(table_gender)

#Test whether the repsonses are different from the random guessing
table_gender_p = xtabs(~ value, data = gender_new)
p = rep(1/2, 2)
chisq.test(table_gender_p,p=p)
```

```{r Study 1 graphic}
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("con", "n"), 
                        c("incon", "n"), 
                        c("con", "incon"))

ggboxplot(gender_time, 
          x = "condition", 
          y = "time",
          color = "condition", 
          palette = "jco",
          add = "jitter")+ 
  theme(legend.position="none")+
  ylab("time (sec)")+
  scale_x_discrete(breaks=c("con","incon","n"),
        labels=c("Congruent","Incongruent","Control"))+
  stat_compare_means(comparisons = my_comparisons, 
                     label.y = c(5.5, 6.5, 7.5))+
  stat_compare_means(label.y = 8.5)
```

The result in Study 1 indicate that both the accuracy (X-Squared=0.70, df=2, p-value=0.71) and the reaction time (F=0.31, df=2, p-value=0.90) do not differ across three conditions. We also prove that responses are significantly different from the random guessing. Therefore, we fail to detect the gender-color bias in the current study. 

### politics
```{r study 2 dataset}
#tidy politics data
p1_time <- tidy(politics1) %>% filter(value ==2)
p2_time <- tidy(politics2) %>% filter(value ==2)
politics_time <- rbind(p1_time, p2_time)
```

```{r study 2 ANOVA}
#anova
politics_aov = lm(time ~ condition, data = politics_time)
fit_politics = Anova(politics_aov, type = 3)
fit_politics
```

```{r study 2 Chi-Square}
#chisq
#rerun function without value selection
p1 <- tidy(politics1)
p2 <- tidy(politics2)
politics_new <- rbind(p1, p2)


table_politics = xtabs(~ condition + value, data = politics_new)
table_politics

chisq.test(table_politics)

#Test whether the repsonses are different from the random guessing
table_p_p = xtabs(~ value, data = politics_new)
p = rep(1/2, 2)
chisq.test(table_p_p,p=p)
```

```{r study 2 graphic}
#Visualize: Specify the comparisons you want
my_comparisons <- list( c("con", "n"), 
                        c("incon", "n"), 
                        c("con", "incon"))

ggboxplot(politics_time, 
          x = "condition", 
          y = "time",
          color = "condition", 
          palette = "jco",
          add = "jitter")+ 
  theme(legend.position="none")+
  ylab("time (sec)")+
  scale_x_discrete(breaks=c("con","incon","n"),
        labels=c("Congruent","Incongruent","Control"))+
  stat_compare_means(comparisons = my_comparisons, 
                     label.y = c(5.5, 6.5, 7.5))+
  stat_compare_means(label.y = 8.5)

```

Result in Study 2 indicate that both the accuracy (X-Squared=0.16, df=2, p-value=0.92) and the reaction time (F=0.27, df=2, p-value=0.76) do not differ across three conditions. We also prove that responses are significantly different from the random guessing. Therefore, we fail to detect the gender-color bias in the current study. 

### emotion
```{r}
#run value selection == 3
e1_time <- tidy(emotion1) %>% filter(value == 3)
#run value selection == 5
e2_time <- tidy(emotion2) %>% filter(value == 5)
```

```{r}
emotion_time <- rbind(e1_time, e2_time)

#anova
emotion_aov = lm(time ~ condition, data = emotion_time)
fit_emotion = Anova(emotion_aov, type = 3)
fit_emotion
```

```{r study 3 Chi-Square}
#chisq test
#rerun function without value selection
e1 <- tidy(emotion1)
e2 <- tidy(emotion2)

table_e1 = xtabs(~ condition + value, data = e1)
table_e2 = xtabs(~ condition + value, data = e2)
table_e1
table_e2

chisq.test(table_e1)
chisq.test(table_e2)

fisher.test(table_e1)
fisher.test(table_e2)

#Test whether the repsonses are different from the random guessing
table_e1p = xtabs(~ value, data = e1)
p = rep(1/6, 6)
chisq.test(table_e1p,p=p)

table_e2p = xtabs(~ value, data = e2)
p = rep(1/6, 6)
chisq.test(table_e2p,p=p)
```

```{r study 3 graphic}
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("con", "n"), 
                        c("incon", "n"), 
                        c("con", "incon"))

ggboxplot(gender_time, 
          x = "condition", 
          y = "time",
          color = "condition", 
          palette = "jco",
          add = "jitter")+ 
  theme(legend.position="none")+
  ylab("time (sec)")+
  scale_x_discrete(breaks=c("con","incon","n"),
        labels=c("Congruent","Incongruent","Control"))+
  stat_compare_means(comparisons = my_comparisons, 
                     label.y = c(5.5, 6.5, 7.5))+
  stat_compare_means(label.y = 8.5)
```

As the choices in study 3 are not binary and the correct answers to question 1 & 2 are different, we do the chi-square test separately. However, the chi-square test indicates that the difference in question 1 is significant (X-Squared=35.02, df=10, p-value<0.001), while the difference in question 2 is not significant (X-Squared=7.46, df=10, p-value=0.68). We also prove that responses are significantly different from the random guessing. One-way ANOVA reveals no difference in reaction time of accurate responses across three conditions (F=0.10, df=2, p-value=0.91).

### degree
```{r}
#run value selection == 2
d1_time <- tidy(degree1) %>% filter(value == 2)
#run value selection == 3
d2_time <- tidy(degree2) %>% filter(value == 3)
```

```{r}
degree_time <- rbind(d1_time, d2_time)
#anova
degree_aov = lm(time ~ condition, data = degree_time)
fit_degree = Anova(degree_aov, type = 3)
fit_degree
```

```{r}
#chisq test
#rerun function without value selection
d1 <- tidy(degree1)
d2 <- tidy(degree2)

table_d1 = xtabs(~ condition + value, data = d1)
table_d2 = xtabs(~ condition + value, data = d2)

chisq.test(table_d1)
chisq.test(table_d2)

#Test whether the repsonses are different from the random guessing
table_d1p = xtabs(~ value, data = d1)
p = rep(1/3, 3)
chisq.test(table_d1p,p=p)

table_d2p = xtabs(~ value, data = d2)
p = rep(1/3, 3)
chisq.test(table_d2p,p=p)
```

```{r}
# Visualize: Specify the comparisons you want
my_comparisons <- list( c("con", "n"), 
                        c("incon", "n"), 
                        c("con", "incon"))

ggboxplot(degree_time, 
          x = "condition", 
          y = "time",
          color = "condition", 
          palette = "jco",
          add = "jitter")+ 
  theme(legend.position="none")+
  ylab("time (sec)")+
  scale_x_discrete(breaks=c("con","incon","n"),
        labels=c("Congruent","Incongruent","Control"))+
  stat_compare_means(comparisons = my_comparisons, 
                     label.y = c(5.5, 6.5, 7.5))+
  stat_compare_means(label.y = 8.5)
```

As the choices in study 4 are not binary and the correct answers to question 1 & 2 are different, we do the chi-square tests separately. However, the chi-square test indicates that the difference in question 1 is not significant (X-Squared=3.85, df=4, p-value=0.43), while the difference in question 2 is significant (X-Squared=14.75, df=4, p-value=0.05). One-way ANOVA reveals no difference in reaction time of accurate responses across three conditions (F=2.03, df=2, p-value=0.14). However, the goodness of fit test proves that responses are significantly different from the random guessing in question 1 but not in study 2.